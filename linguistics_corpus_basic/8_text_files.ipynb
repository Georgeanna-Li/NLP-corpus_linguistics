{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 8: Text Files\n",
    "\n",
    "* Basic reading & writing\n",
    "* Encodings\n",
    "* JSON\n",
    "* Multiple files\n",
    "* Building a simple corpus reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have primarily been working with text files in the context of specific, complex formats, using Python packages like Pandas (for CSVs) and Beautiful Soup (for XML). One advantage of these packages is that they will deal with messy things for you, and get things set up fast.\n",
    "\n",
    "However, we will inevitably run across a situation where these aren't quite the right thing, for instance:\n",
    "\n",
    "* What about raw English text, with no formatting?\n",
    "    * To be processed into sentences, tokens, perhaps tagged?\n",
    "    * or for information to be extracted with regular expressions\n",
    "* What about one sentence/word per line?\n",
    "* What about CSV (or TSV) type situations but where you want to process a line at a time (not load the entire file)\n",
    "* What about storing lexicons?\n",
    "* What about working in other languages with different character sets?\n",
    "* What happens when I've got a bunch of text files to process at once?\n",
    "\n",
    "In this notebook, we'll dig a bit deeper into text file I/O (Input/Output) in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic reading and writing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The classic method for opening files in Python is to assign a file object (f) to the result of the open function, and then close the file by calling the close method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"cells\": [\n",
      "  {\n",
      "   \"cell_type\": \"markdown\",\n",
      "   \"metadata\": {\n",
      "    \"slideshow\": {\n",
      "     \"slide_type\": \"slide\"\n",
      "    }\n",
      "   },\n",
      "   \"source\": [\n",
      "    \"# 8: Text Files\\n\",\n",
      "    \"\\n\",\n",
      "    \"* Basic reading & writing\\n\",\n",
      "    \"* Encodings\\n\",\n",
      "    \"* JSON\\n\",\n",
      "    \"* Multiple files\\n\",\n",
      "    \"* Building a simple corpus reader\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"markdown\",\n",
      "   \"metadata\": {},\n",
      "   \"source\": [\n",
      "    \"So far, we have primarily been working with text files in the context of specific, complex formats, using Pyth\n"
     ]
    }
   ],
   "source": [
    "f = open(\"8_text_files.ipynb\")\n",
    "print(f.read(500))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An popular recent alternative method is to use Python with...as syntax, which will close the file automatically at the end of the code block. \n",
    "\n",
    "- Advantage: won't accidently leave files open and lose data\n",
    "- Disadvantage: extra indentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"cells\": [\n",
      "  {\n",
      "   \"cell_type\": \"markdown\",\n",
      "   \"metadata\": {\n",
      "    \"slideshow\": {\n",
      "     \"slide_type\": \"slide\"\n",
      "    }\n",
      "   },\n",
      "   \"source\": [\n",
      "    \"# 8: Text Files\\n\",\n",
      "    \"\\n\",\n",
      "    \"* Basic reading & writing\\n\",\n",
      "    \"* Encodings\\n\",\n",
      "    \"* JSON\\n\",\n",
      "    \"* Multiple files\\n\",\n",
      "    \"* Building a simple corpus reader\"\n",
      "   ]\n",
      "  },\n",
      "  {\n",
      "   \"cell_type\": \"markdown\",\n",
      "   \"metadata\": {},\n",
      "   \"source\": [\n",
      "    \"So far, we have primarily been working with text files in the context of specific, complex formats, using Pyth\n"
     ]
    }
   ],
   "source": [
    "with open(\"8_text_files.ipynb\") as f:\n",
    "    print(f.read(500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Remember that \"r\" mode does not need to be specified for reading, it is the default mode. Write mode \"w\" overwrites the file you are creating. The append option \"a\" can be useful if you are adding continuously to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"test.txt\",\"w\") as fout:\n",
    "    fout.write(\"test write 1\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.txt\",\"w\",) as fout:\n",
    "    fout.write(\"test write 2\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.txt\",\"a\") as fout:\n",
    "    fout.write(\"test append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test write 2\n",
      "test append\n"
     ]
    }
   ],
   "source": [
    "f = open(\"test.txt\")\n",
    "print(f.read())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The two most common options for reading files are iterating line by line using a *for* loop (which does not require holding the entire file in memory), or reading the entire file into a single string at once, using [read](https://docs.python.org/3/library/io.html#io.TextIOBase.read) (great for quick use of regexes, for instance). You can read a single line of a file without loop by using [readline](https://docs.python.org/3/library/io.html#io.TextIOBase.readline). A fourth option is [readlines](https://docs.python.org/3/library/io.html#io.IOBase.readlines), which will read the entire file into a list of string where each string is a line. Remember that in all of these cases, the newline characters will still be there, so you'll probably want to use strip!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "some_lines = \"line1\\nline2\\nline3\\nline4\"\n",
    "with open(\"test.txt\",\"w\") as fout:\n",
    "    fout.write(some_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line1\n",
      "line2\n",
      "line3\n",
      "line4\n"
     ]
    }
   ],
   "source": [
    "with open(\"test.txt\") as f:\n",
    "    # my code here\n",
    "    print(f.read())\n",
    "    # my code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line1\n",
      "line2\n",
      "line3\n",
      "line4\n"
     ]
    }
   ],
   "source": [
    "with open(\"test.txt\") as f:\n",
    "    # my code here\n",
    "    for line in f:\n",
    "        print(line.strip())\n",
    "    # my code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line1\n",
      "\n",
      "line2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"test.txt\") as f:\n",
    "    # my code here\n",
    "    print(f.readline())\n",
    "    print(f.readline())\n",
    "    # my code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['line1\\n', 'line2\\n', 'line3\\n', 'line4']\n"
     ]
    }
   ],
   "source": [
    "with open(\"test.txt\") as f:\n",
    "    # my code here\n",
    "    print(f.readlines())\n",
    "    # my code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For writing, the [write] method can be used whether you are writing incrementally or one shot. (There is a also [writelines](https://docs.python.org/3/library/io.html#io.IOBase.writelines) method if you already have a list of strings, though note that newlines are not added. This is less used though.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_lines = [\"line1\",\"line2\",\"line3\",\"line4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "with open(\"test.txt\",\"w\") as fout:\n",
    "    #my code here\n",
    "    for line in some_lines:\n",
    "        fout.write(line + \"\\n\")\n",
    "    #my code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line1\n",
      "line2\n",
      "line3\n",
      "line4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"test.txt\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For practice, let's write and then read in the counts of words in the Brown, using a tab to deliminate the word and its count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "from collections import Counter\n",
    "\n",
    "counts = Counter(brown.words())\n",
    "\n",
    "counts_copy = {}\n",
    "\n",
    "#my code here\n",
    "fout = open(\"brown_counts.txt\", \"w\")\n",
    "for word, count in counts.items():\n",
    "    fout.write(word + \"\\t\" + str(count) + \"\\n\")\n",
    "fout.close()\n",
    "\n",
    "f = open(\"brown_counts.txt\")\n",
    "for line in f:\n",
    "    word, count = line.strip().split()\n",
    "    counts_copy[word] = int(count)\n",
    "f.close()\n",
    "#my code here\n",
    "\n",
    "counts == counts_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a lexicon or corpus in CSV/TSV involving more than multiple columns, you probably won't want to manipulate each line manually like we did here, however, but you may not want to use pandas either, which requires you load the entire file into memory. \n",
    "\n",
    "An intermediate option between those two extremes is to use Python's [csv](https://docs.python.org/3/library/csv.html) library, which is lightweight but easy to use (note that it will do any necessary escaping for you!). For example, one popular format for annotated corpora is the CoNLL format, an example with headers is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID\tFORM\tLEMMA\tUPOSTAG\tXPOSTAG\tFEATS\tHEAD\tDEPREL\n",
      "1\tHe\the\tPRON\tPRP\tCase=Nom|Number=Sing|Person=3\t2\tnsubj\n",
      "2\tis\tbe\tVERB\tVBZ\tNumber=Sing|Person=3|Tense=Pres\t0\troot\n",
      "3\tin\tin\tADP\tIN\t_\t6\tcase\n",
      "4\tthe\tthe\tDET\tDT\tDefinite=Def|PronType=Art\t6\tdet\n",
      "5\tUnited\tunite\tVERB\tVBD\tTense=Past|VerbForm=Part\t6\tamod\n",
      "6\tKingdom\tkingdom\tNOUN\tNN\tNumber=Sing\t2   nmod\n",
      "7\t(\t(\tPUNCT\t-LRB-\t_\t8\tpunct\n",
      "8\tUK\tUK\tPROPN\tNNP\tNumber=Sing\t6\tappos\n",
      "9\t)\t)\tPUNCT\t-RRB-\t_\t8\tpunct\n",
      "10\t.\t.\tPUNCT\t.\t_\t2\tpunct\n"
     ]
    }
   ],
   "source": [
    "CoNLL_example = \"ID\\tFORM\\tLEMMA\\tUPOSTAG\\tXPOSTAG\\tFEATS\\tHEAD\\tDEPREL\\n1\\tHe\\the\\tPRON\\tPRP\\tCase=Nom|Number=Sing|Person=3\\t2\\tnsubj\\n2\\tis\\tbe\\tVERB\\tVBZ\\tNumber=Sing|Person=3|Tense=Pres\\t0\\troot\\n3\\tin\\tin\\tADP\\tIN\\t_\\t6\\tcase\\n4\\tthe\\tthe\\tDET\\tDT\\tDefinite=Def|PronType=Art\\t6\\tdet\\n5\\tUnited\\tunite\\tVERB\\tVBD\\tTense=Past|VerbForm=Part\\t6\\tamod\\n6\\tKingdom\\tkingdom\\tNOUN\\tNN\\tNumber=Sing\\t2   nmod\\n7\\t(\\t(\\tPUNCT\\t-LRB-\\t_\\t8\\tpunct\\n8\\tUK\\tUK\\tPROPN\\tNNP\\tNumber=Sing\\t6\\tappos\\n9\\t)\\t)\\tPUNCT\\t-RRB-\\t_\\t8\\tpunct\\n10\\t.\\t.\\tPUNCT\\t.\\t_\\t2\\tpunct\"\n",
    "with open(\"CoNNL.txt\",\"w\") as fout:\n",
    "    fout.write(CoNLL_example)\n",
    "print(CoNLL_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we like, we can skip the header line and read each row using a simple CSV [reader](https://docs.python.org/3/library/csv.html#csv.reader), which returns a list for each row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', 'He', 'he', 'PRON', 'PRP', 'Case=Nom|Number=Sing|Person=3', '2', 'nsubj']\n",
      "['2', 'is', 'be', 'VERB', 'VBZ', 'Number=Sing|Person=3|Tense=Pres', '0', 'root']\n",
      "['3', 'in', 'in', 'ADP', 'IN', '_', '6', 'case']\n",
      "['4', 'the', 'the', 'DET', 'DT', 'Definite=Def|PronType=Art', '6', 'det']\n",
      "['5', 'United', 'unite', 'VERB', 'VBD', 'Tense=Past|VerbForm=Part', '6', 'amod']\n",
      "['6', 'Kingdom', 'kingdom', 'NOUN', 'NN', 'Number=Sing', '2   nmod']\n",
      "['7', '(', '(', 'PUNCT', '-LRB-', '_', '8', 'punct']\n",
      "['8', 'UK', 'UK', 'PROPN', 'NNP', 'Number=Sing', '6', 'appos']\n",
      "['9', ')', ')', 'PUNCT', '-RRB-', '_', '8', 'punct']\n",
      "['10', '.', '.', 'PUNCT', '.', '_', '2', 'punct']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "f = open(\"CoNNL.txt\")\n",
    "f.readline() # skip the header by reading one line\n",
    "reader = csv.reader(f,delimiter=\"\\t\")\n",
    "for row in reader:\n",
    "    print(row)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we might use a [DictReader](https://docs.python.org/3/library/csv.html#csv.DictReader), where each row is a dictionary with the headers as keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ID': '1', 'FORM': 'He', 'LEMMA': 'he', 'UPOSTAG': 'PRON', 'XPOSTAG': 'PRP', 'FEATS': 'Case=Nom|Number=Sing|Person=3', 'HEAD': '2', 'DEPREL': 'nsubj'}\n",
      "{'ID': '2', 'FORM': 'is', 'LEMMA': 'be', 'UPOSTAG': 'VERB', 'XPOSTAG': 'VBZ', 'FEATS': 'Number=Sing|Person=3|Tense=Pres', 'HEAD': '0', 'DEPREL': 'root'}\n",
      "{'ID': '3', 'FORM': 'in', 'LEMMA': 'in', 'UPOSTAG': 'ADP', 'XPOSTAG': 'IN', 'FEATS': '_', 'HEAD': '6', 'DEPREL': 'case'}\n",
      "{'ID': '4', 'FORM': 'the', 'LEMMA': 'the', 'UPOSTAG': 'DET', 'XPOSTAG': 'DT', 'FEATS': 'Definite=Def|PronType=Art', 'HEAD': '6', 'DEPREL': 'det'}\n",
      "{'ID': '5', 'FORM': 'United', 'LEMMA': 'unite', 'UPOSTAG': 'VERB', 'XPOSTAG': 'VBD', 'FEATS': 'Tense=Past|VerbForm=Part', 'HEAD': '6', 'DEPREL': 'amod'}\n",
      "{'ID': '6', 'FORM': 'Kingdom', 'LEMMA': 'kingdom', 'UPOSTAG': 'NOUN', 'XPOSTAG': 'NN', 'FEATS': 'Number=Sing', 'HEAD': '2   nmod', 'DEPREL': None}\n",
      "{'ID': '7', 'FORM': '(', 'LEMMA': '(', 'UPOSTAG': 'PUNCT', 'XPOSTAG': '-LRB-', 'FEATS': '_', 'HEAD': '8', 'DEPREL': 'punct'}\n",
      "{'ID': '8', 'FORM': 'UK', 'LEMMA': 'UK', 'UPOSTAG': 'PROPN', 'XPOSTAG': 'NNP', 'FEATS': 'Number=Sing', 'HEAD': '6', 'DEPREL': 'appos'}\n",
      "{'ID': '9', 'FORM': ')', 'LEMMA': ')', 'UPOSTAG': 'PUNCT', 'XPOSTAG': '-RRB-', 'FEATS': '_', 'HEAD': '8', 'DEPREL': 'punct'}\n",
      "{'ID': '10', 'FORM': '.', 'LEMMA': '.', 'UPOSTAG': 'PUNCT', 'XPOSTAG': '.', 'FEATS': '_', 'HEAD': '2', 'DEPREL': 'punct'}\n"
     ]
    }
   ],
   "source": [
    "f = open(\"CoNNL.txt\")\n",
    "reader = csv.DictReader(f,delimiter=\"\\t\")\n",
    "for row in reader:\n",
    "    print(row)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, one other structured text format that is popular in computational liguistics and beyond is the JSON. As it happens, .ipynb is a JSON format, so we can read these lecture notes, using [json.load](https://docs.python.org/3/library/json.html#json.load), which can be passed a filepointer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "f = open(\"8_text_files.ipynb\")\n",
    "lecture_8 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cell_type': 'markdown',\n",
       " 'metadata': {'slideshow': {'slide_type': 'slide'}},\n",
       " 'source': ['# 8: Text Files\\n',\n",
       "  '\\n',\n",
       "  '* Basic reading & writing\\n',\n",
       "  '* Encodings\\n',\n",
       "  '* JSON\\n',\n",
       "  '* Multiple files\\n',\n",
       "  '* Building a simple corpus reader']}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lecture_8['cells'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This probably won't work on Windows systems but it works on my Mac. Since Mac and PCs have different default encodings, let's change topics and first talk about encodings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For computers, numbers are everything. However, when dealing with texts, we need a way to associate numbers with characters. *Encodings* provide such a mapping. Generally there is trade-off between the number of possible characters that can be represented and the amount of space required to store text on disk, so different encodings were developed so they could represent the particular characters used in particular languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](http://www.asciitable.com/index/asciifull.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, an encoding can be selecting using the `encoding` keyword when you open a file. ASCII was the first major encoding and is very compact but can only represent 128 characters; using ASCII will fail if you try to write text that uses characters which aren't found on typical English keyboard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'ascii' codec can't encode character '\\xe7' in position 0: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-15b7f416f7e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ascii\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"this works\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mfout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ça ne va pas\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mfout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"不行\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode character '\\xe7' in position 0: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "with open(\"test.txt\", \"w\",encoding=\"ascii\") as fout:\n",
    "    fout.write(\"this works\\n\")\n",
    "    fout.write(\"ça ne va pas\\n\")\n",
    "    fout.write(\"不行\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Latin-1 and various related formats can use up to 256 characters (a full byte), and support most of the languages of Europe. A variation on Latin-1 called CP-1252 is usually the default encoding for Windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'latin-1' codec can't encode characters in position 0-3: ordinal not in range(256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-1fa1dd976c83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"this works\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ça va\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mfout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"还是不行\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'latin-1' codec can't encode characters in position 0-3: ordinal not in range(256)"
     ]
    }
   ],
   "source": [
    "with open(\"test.txt\", \"w\",encoding=\"latin-1\") as fout:\n",
    "    fout.write(\"this works\")\n",
    "    fout.write(\"ça va\")\n",
    "    fout.write(\"还是不行\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "These days, the most popular encoding is definitely UTF-8, which supports all the characters included in Unicode, including all the characters of pretty much every written language, as well as things like emoji. Even if you don't think you need it, it is a good idea to save the text files you create to be in UTF-8. The characters included in ASCII have the same representation in UTF-8, so for normal English texts it is actually no less efficient! Note that UTF-8 is the default encoding for OS X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"test.txt\", \"w\",encoding=\"utf-8\") as fout:\n",
    "    fout.write(\"this works\\n\")\n",
    "    fout.write(\"ça va\\n\")\n",
    "    fout.write(\"可以了\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this works\n",
      "ça va\n",
      "可以了\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"test.txt\",encoding=\"utf-8\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xc3 in position 11: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-8bd2e97e9302>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ascii\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/miniconda3/lib/python3.9/encodings/ascii.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascii_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xc3 in position 11: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "with open(\"test.txt\",encoding=\"ascii\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Most of the time, encodings just work and you don't have to think about them. However, sooner or later (probably sooner) you will get an encoding error when you read a file. You might try changing the encoding, or trying to autodetect the encoding (BeautifulSoup [can help you with this](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#unicode-dammit)). But sometimes it just doesn't work (or you don't have the patience), at which point you might want to try a more liberal option for the *errors* keyword parameter such as ignore or replace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this works��a va���������\n"
     ]
    }
   ],
   "source": [
    "with open(\"test.txt\", \"w\",encoding=\"utf-8\") as fout:\n",
    "    fout.write(\"this works\")\n",
    "    fout.write(\"ça va\")\n",
    "    fout.write(\"可以了\")\n",
    "    \n",
    "    \n",
    "with open(\"test.txt\",encoding=\"ascii\",errors=\"replace\") as f:\n",
    "    print(f.read())\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Finally, although encodings can be handled as part of file IO, sometimes you need to [encode](https://docs.python.org/3/library/stdtypes.html#str.encode) to a bytes string or [decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode) from a bytes (binary) string when there is no file directly involved. Use the encode and decode methods for strings, with also have the errors keyword argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'this works\\xc3\\xa7a va\\xe5\\x8f\\xaf\\xe4\\xbb\\xa5\\xe4\\xba\\x86'\n",
      "this worksça va可以了\n",
      "this worksa va\n"
     ]
    }
   ],
   "source": [
    "with open(\"test.txt\",\"rb\") as f:\n",
    "    text = f.read()\n",
    "    print(text)\n",
    "    print(text.decode(\"utf-8\"))\n",
    "    print(text.decode(\"ascii\",errors=\"ignore\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand encodings, let's try that JSON example again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"8_text_files.ipynb\", encoding=\"utf-8\")\n",
    "lecture_8 = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JSON format represents all the basic Python types, including strings, ints, lists, and dicts. Let's take a look what we have after we've loaded in from that .ipynb JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lecture_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['cells', 'metadata', 'nbformat', 'nbformat_minor'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lecture_8.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cell_type': 'markdown',\n",
       " 'metadata': {'slideshow': {'slide_type': 'slide'}},\n",
       " 'source': ['# 8: Text Files\\n',\n",
       "  '\\n',\n",
       "  '* Basic reading & writing\\n',\n",
       "  '* Encodings\\n',\n",
       "  '* JSON\\n',\n",
       "  '* Multiple files\\n',\n",
       "  '* Building a simple corpus reader']}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lecture_8[\"cells\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working in reverse, we can use JSONs to store any complex python data structure w have constructed. There are more compact ways to store these sorts of objects using a binary representation rather than text (e.g. [pickle](https://docs.python.org/3/library/pickle.html)), but JSON has the advantage of being easy to read. Write to a file using the [dump](https://docs.python.org/3/library/json.html#json.dump) function after you've opened a file for writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fout = open(\"8_text_files.ipynb\",\"w\",encoding=\"utf-8\")\n",
    "json.dump(lecture_8, fout)\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiple files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with corpora, we will often want to iterate over multiple files, applying the same processing to all of them. The easiest way to accomplish this is to use [os.listdir](https://docs.python.org/3/library/os.html#os.listdir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CoNNL.txt',\n",
       " '.DS_Store',\n",
       " '5_regex.ipynb',\n",
       " '3_lexicons.ipynb',\n",
       " '6_XML.ipynb',\n",
       " '2_corpora.ipynb',\n",
       " 'brown_counts.txt',\n",
       " '1_strings.ipynb',\n",
       " 'test.txt',\n",
       " '.ipynb_checkpoints',\n",
       " '4_stats.ipynb',\n",
       " '8_text_files.ipynb',\n",
       " '7_preprocess.ipynb']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's programmatically make some files in a subdirectory directory for us to read. We use [os.mkdir](https://docs.python.org/3/library/os.html#os.mkdir) for this (note if you run this code more than once, you'll have to comment out os.mkdir because you can only make the directory once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test1.txt', 'test2.txt']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.mkdir(\"files\")\n",
    "fout = open(\"files/test1.txt\",\"w\")\n",
    "fout.write(\"Hello World!\")\n",
    "fout.close()\n",
    "fout = open(\"files/test2.txt\",\"w\")\n",
    "fout.write(\"Goodbye World!\")\n",
    "fout.close()\n",
    "os.listdir(\"files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can iterate over the list of filenames and open each one, concatenating the directory name to the front to form a path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n",
      "Goodbye World!\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(\"files\"):\n",
    "    with open(\"files/\" + filename) as f:\n",
    "        print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular way to deal with multiple files is to keep them in archives, so they take up minimal space and are easy to transfer. All major archive formats that have direct library support in Python, including [zip](https://docs.python.org/3/library/zipfile.html), [gzip](https://docs.python.org/3/library/gzip.html), [tar](https://docs.python.org/3/library/tarfile.html), and [bzip2](https://docs.python.org/3/library/bz2.html). The code below creates a zip file with the two text files we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "my_zip = ZipFile(\"test.zip\",\"w\")\n",
    "my_zip.write(\"files/test1.txt\")\n",
    "my_zip.write(\"files/test2.txt\")\n",
    "my_zip.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could extract those files, but a more common use case is to access them directly inside the zip. We open the zipfile, and iterate over its contents using [ZipFile.namelist](https://docs.python.org/3/library/zipfile.html#zipfile.ZipFile.namelist), and get a filepointer for the archived file using [ZipFile.open](https://docs.python.org/3/library/zipfile.html#zipfile.ZipFile.open):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files/test1.txt\n",
      "b'Hello World!'\n",
      "files/test2.txt\n",
      "b'Goodbye World!'\n"
     ]
    }
   ],
   "source": [
    "my_zip = ZipFile(\"test.zip\")\n",
    "for filename in my_zip.namelist():\n",
    "    print(filename)\n",
    "    f = my_zip.open(filename)\n",
    "    print(f.read())\n",
    "    f.close()\n",
    "my_zip.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the b in front of the strings, indicating they are bytes strings, not (normal) Unicode strings; ZipFile opens files in binary mode. To convert them to a \"proper\" Python Unicode string, we would use the `decode` method mentioned above with the correct encoding (in this case, any of the encodings from above would work)! Note you will run across the same problem you manipulate a webpage directly rather than passing it to Beautiful Soup, which does the conversion for you. You will definitely want to decode to a correct Unicode string, a bytes string is not what you normally want to work with!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a simple corpus reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early in the repo we were able to explore corpora without dealing with the details of file I/O and text preprocessing because the NLTK corpus readers allow us to sidestep those issues.\n",
    "\n",
    "Now, we have all the tools in place to build our own Corpus Readers. We'll now build a class that has a `words` method that acts just like `words` does for corpora in NLTK. \n",
    "\n",
    "In addition to some stuff we've learned recently, we'll also need our function to be a generator. Good corpus readers should not load the entire corpus into memory, they should `yield`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "import os\n",
    "\n",
    "class MyCorpusReader: \n",
    "    def __init__(self, directory, encoding):\n",
    "        self.directory = directory\n",
    "        self.encoding = encoding\n",
    "        \n",
    "    def words(self):\n",
    "        # my code here\n",
    "        for filename in os.listdir(self.directory):\n",
    "            f = open(self.directory + \"/\" + filename, encoding=self.encoding)\n",
    "            text = f.read()\n",
    "            words = word_tokenize(text)\n",
    "            for word in words:\n",
    "                yield word\n",
    "        # my code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Hello\n",
      "1\n",
      "World\n",
      "2\n",
      "!\n",
      "3\n",
      "Goodbye\n",
      "4\n",
      "World\n",
      "5\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "reader = MyCorpusReader(\"files\", \"utf-8\")\n",
    "for i,word in enumerate(reader.words()):\n",
    "    print(i)\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We could easily extend this to include all the functionality we got in NLTK (sents, tagged_sents, etc.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
