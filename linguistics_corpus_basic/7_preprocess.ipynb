{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 7: Text Preprocessing\n",
    "\n",
    "* Sentence segmentation\n",
    "* Tokenization\n",
    "* Lemmatization and Stemming\n",
    "* POS tagging\n",
    "* End-to-end preprocessing with SpaCy\n",
    "* T/F questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sentence segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For many computational linguistics applications, the sentence is a key unit of processing. All modern written languages have an end-of-sentence marker. For some languages, like Chinese, this marker is unambigious and so getting the sentences of the text is very easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['你好', '我叫家瑞', '我是老师', '']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_text = \"你好。我叫家瑞。我是老师。\"\n",
    "\n",
    "zh_sents = zh_text.split(\"。\")\n",
    "zh_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, English is not so easy because the period (\".\") is ambiguous. It has various uses. This is true for many other European languages as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Garrett got his Ph\n",
      "D\n",
      " from the Univ\n",
      " of Alberta in 07/2017\n",
      " Dr\n",
      " Nicolai's thesis was too long, as they tend to be\n",
      " He went to the U\n",
      "S\n",
      "A\n",
      " \n",
      "\n",
      "\n",
      " but later came back to Canada\n",
      "\n"
     ]
    }
   ],
   "source": [
    "en_text = \"Garrett got his Ph.D. from the Univ. of Alberta in 07/2017. Dr. Nicolai's thesis was too long, as they tend to be. He went to the U.S.A. ... but later came back to Canada.\"\n",
    "\n",
    "for sent in en_text.split(\".\"):\n",
    "    print(sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are ways to improve things considerably by using regular expressions; for instance, splitting only when a period is followed by a space and then an upper case letter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Garrett got his Ph.D. from the Univ. of Alberta in 07/2017',\n",
       " \"Dr. Nicolai's thesis was too long, as they tend to be\",\n",
       " 'He went to the U.S.A. ... but later came back to Canada.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "regex = r\"(?<!Dr)\\. (?=[A-Z])\"\n",
    "\n",
    "re.split(regex, en_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes: Here the ?<! is negative `lookback`, so if there are Dr. something, we don't split the sentence from there(as we can observe there, Dr. Nicolai's is not splitted).**\n",
    "\n",
    "**But the ?= is positive `lookahead`(the thing we are trying to get is ahead). We are saying: we only want to put a split there only if the space is followed by some capital letters.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For major languages with significant ambiguity, you'll probably want to use a dedicated sentence splitter, which NLTK has for some languages (17). But don't expect perfection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Garrett got his Ph.D. from the Univ.',\n",
       " 'of Alberta in 07/2017.',\n",
       " \"Dr. Nicolai's thesis was too long, as they tend to be.\",\n",
       " 'He went to the U.S.A. ... but later came back to Canada.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "sent_tokenize(en_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Again, breaking a sentence up into words seems like it should be easy, but actually rarely is. Spaces separate most word tokens, but punctuation is a problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Dr. Nicolai's thesis was too long, as they tend to be.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = sent_tokenize(en_text)\n",
    "sent = sents[2]\n",
    "sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr.',\n",
       " \"Nicolai's\",\n",
       " 'thesis',\n",
       " 'was',\n",
       " 'too',\n",
       " 'long,',\n",
       " 'as',\n",
       " 'they',\n",
       " 'tend',\n",
       " 'to',\n",
       " 'be.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr',\n",
       " 'Nicolai',\n",
       " 's',\n",
       " 'thesis',\n",
       " 'was',\n",
       " 'too',\n",
       " 'long',\n",
       " 'as',\n",
       " 'they',\n",
       " 'tend',\n",
       " 'to',\n",
       " 'be']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[match.group() for match in re.finditer(\"\\w+\", sent)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In computational linguistics for English, *clitics* (small words that are phonologically joined to a host word) such as \"n't\" and \"'s\" are often treated as separate words, and need to be dealt with specially). A proper word tokenizer (such as is included in NLTK) will deal with these subtle issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr.',\n",
       " 'Nicolai',\n",
       " \"'s\",\n",
       " 'thesis',\n",
       " 'was',\n",
       " 'too',\n",
       " 'long',\n",
       " ',',\n",
       " 'as',\n",
       " 'they',\n",
       " 'tend',\n",
       " 'to',\n",
       " 'be',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "word_tokenize(sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In some languages (such as Chinese) there are no spaces in the words. Much more sophisticated word segmenters are needed in this case, for example [jieba](https://github.com/fxsjy/jieba) for Chinese (you'll need to install the package to run this code). One challenge for these languages is that it isn't always clear what a word is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\nicol\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.929 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词 是 小菜一碟\n"
     ]
    }
   ],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m install jieba\n",
    "from jieba import cut\n",
    "zh_text = \"分词是小菜一碟\" # tokenization is a piece of cake\n",
    "\n",
    "print(\" \".join(cut(zh_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lemmatization and Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is the process of producing morphological variants of a root/base word. A stemming algorithm reduces the words “chocolates”, “chocolatey”, “choco” to the root word, “chocolate” and “retrieval”, “retrieved”, “retrieves” reduce to the stem “retrieve”. Stemming is an important part of the pipelining process in Natural language processing. The input to the stemmer is tokenized words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applications of stemming : \n",
    "- Stemming is used in information retrieval systems like search engines.\n",
    "- It is used to determine domain vocabularies in domain analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes: refer to [this StackOverflow for reference](https://stackoverflow.com/questions/1787110/what-is-the-difference-between-lemmatization-vs-stemming)**\n",
    "\n",
    "Stemming just removes or stems the last few characters of a word, often leading to incorrect meanings and spelling. Lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma. Sometimes, the same word can have multiple different Lemmas. We should identify the Part of Speech (POS) tag for the word in that specific context. Here are the examples to illustrate all the differences and use cases:\n",
    "\n",
    "1. If you lemmatize the word 'Caring', it would return 'Care'. If you stem, it would return 'Car' and this is erroneous.\n",
    "\n",
    "2. If you lemmatize the word 'Stripes' in verb context, it would return 'Strip'. If you lemmatize it in noun context, it would return 'Stripe'. If you just stem it, it would just return 'Strip'.\n",
    "\n",
    "3. You would get same results whether you lemmatize or stem words such as walking, running, swimming... to walk, run, swim etc.\n",
    "\n",
    "4. Lemmatization is computationally expensive since it involves look-up tables and what not. If you have large dataset and performance is an issue, go with Stemming. Remember you can also add your own rules to Stemming. If accuracy is paramount and dataset isn't humongous, go with Lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For some applications, it can be useful to ignore the morphological differences between words. A classic example is information retrival (i.e. web search); if a user looks for \"sleeping kitties\", you might want to include in your results a page which mentions that \"the kitty sleeps\". By default, though, \"kitty\" and \"kitties\" and \"sleeps\" and \"sleeping\" are completely different word types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "text = [\"The\", \"kitty\", \"sleeps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"sleeping\" in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"kitties\" in text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lemmatization converts an inflected form to a base, uninflected form. This decreases the size of your vocabulary (eliminating rare forms), and is often useful if you want calculate statistics using lexicons but don't want to store all the possible forms.  NLTK has a lemmatizer for English that uses the WordNet lexicon. One tricky aspect is that by default it requires a part-of-speech. It works for both regular and irregular forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer= WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitty'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"kitties\", \"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sleep'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"sleeping\", \"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sleeping'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"sleeping\", \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'woman'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"women\", \"n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sleep'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"sleep\", \"v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't want to POS tag, then trying to lemmatize as a noun first, then a verb, will work in most cases, but not all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word, \"n\")\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word, \"v\")\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitty'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(\"kitties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sleep'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(\"sleeping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doe'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(\"does\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'do'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"does\", \"v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Stemming has a similar purpose but it strips off both inflectional and derivational mophology to reach a stem. This stem is not always itself a word. Sometime stemming incorrectly collapses/conflates words with very different meaning, or fails to collapse related words. The most popular stemming algorithm for English is called the [porter stemmer](http://snowball.tartarus.org/algorithms/porter/stemmer.html), it involves a bunch of rewrite rules to get rid of common suffixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sleep'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"sleeping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitti'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"kitty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitti'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"kitties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "S1 = \"automatization\"\n",
    "S2 = \"automatic\"\n",
    "S3 = \"has\"\n",
    "S4 = \"have\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'automat'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(S1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'automat'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(S2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ha'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(S3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(S4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's try both lemmatizing and stemming the sentence below (after tokenizing), and compare the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Whereas', 'disregard', 'and', 'contempt', 'for', 'human', 'right', 'have', 'result', 'in', 'barbarous', 'act', 'which', 'have', 'outrage', 'the', 'conscience', 'of', 'mankind', ',', 'and', 'the', 'advent', 'of', 'a', 'world', 'in', 'which', 'human', 'being', 'shall', 'enjoy', 'freedom', 'of', 'speech', 'and', 'belief', 'and', 'freedom', 'from', 'fear', 'and', 'want', 'ha', 'be', 'proclaim', 'a', 'the', 'highest', 'aspiration', 'of', 'the', 'common', 'people']\n",
      "['wherea', 'disregard', 'and', 'contempt', 'for', 'human', 'right', 'have', 'result', 'in', 'barbar', 'act', 'which', 'have', 'outrag', 'the', 'conscienc', 'of', 'mankind', ',', 'and', 'the', 'advent', 'of', 'a', 'world', 'in', 'which', 'human', 'be', 'shall', 'enjoy', 'freedom', 'of', 'speech', 'and', 'belief', 'and', 'freedom', 'from', 'fear', 'and', 'want', 'ha', 'been', 'proclaim', 'as', 'the', 'highest', 'aspir', 'of', 'the', 'common', 'peopl']\n"
     ]
    }
   ],
   "source": [
    "S = \"Whereas disregard and contempt for human rights have resulted in barbarous acts which have outraged the conscience of mankind, and the advent of a world in which human beings shall enjoy freedom of speech and belief and freedom from fear and want has been proclaimed as the highest aspiration of the common people\"\n",
    "words = word_tokenize(S)\n",
    "lemmas = [lemmatize(word) for word in words]\n",
    "print(lemmas)\n",
    "stems = [stemmer.stem(word) for word in words]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We've already seen that POS tagging can be useful for doing analysis of corpora. It has other uses, for instance it can be used to focus on particular kinds of words for certain applications, and it can provide simple word sense disambiguation (e.g. the word \"cross\"). The NLTK POS tagger for English is easy to use and effective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nicol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('think', 'VBP'), ('the', 'DT'), ('NLTK', 'NNP'), ('pos', 'NN'), ('tagger', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('pretty', 'RB'), ('solid', 'JJ'), ('tool', 'NN'), ('for', 'IN'), ('doing', 'VBG'), ('computational', 'JJ'), ('linguistics', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "S = \"I think the NLTK pos tagger is a pretty solid tool for doing computational linguistics\"\n",
    "print(pos_tag(S.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The standard tagset for English and used for the NLTK tagger (and most others) is the one created as part of the Penn Treebank annotation project, defined [here](https://www.anc.org/penn.html). We can also POS tag using the universal tagset if we like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\nicol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRON'),\n",
       " ('think', 'VERB'),\n",
       " ('the', 'DET'),\n",
       " ('NLTK', 'NOUN'),\n",
       " ('pos', 'NOUN'),\n",
       " ('tagger', 'NOUN'),\n",
       " ('is', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('pretty', 'ADV'),\n",
       " ('solid', 'ADJ'),\n",
       " ('tool', 'NOUN'),\n",
       " ('for', 'ADP'),\n",
       " ('doing', 'VERB'),\n",
       " ('computational', 'ADJ'),\n",
       " ('linguistics', 'NOUN')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(S.split(\" \"), tagset=\"universal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can also use NLTK to build a POS tagger for any language where we have a manually tagged corpus and/or morphological information which can be expressed in the form of regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('he', 'PRP'), ('googled', 'VBD'), ('cats', 'NNS')]\n",
      "[('he', 'PRP'), ('googled', 'VBD'), ('cats', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import RegexpTagger, UnigramTagger\n",
    "from nltk.corpus import treebank \n",
    "\n",
    "patterns = [(r\"\\d+\", \"CD\"),(r\".*ing$\", \"VBG\"), (r\".*ed$\", \"VBD\"),(r\".*s$\", \"NNS\"),(r\".*\", \"NN\")]\n",
    "sentence = [\"he\", \"googled\", \"cats\"]\n",
    "tagged = treebank.tagged_sents()\n",
    "\n",
    "re_tagger= RegexpTagger(patterns)\n",
    "uni_tagger= UnigramTagger(tagged, backoff=re_tagger)\n",
    "print(pos_tag(sentence))\n",
    "print(uni_tagger.tag(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's use NLTK to tokenize the sentences about me, and then, for each sentence, compare the tags from the main NLTK POS tagger and the simple one we just built. Are there many differences? When there are, which do you think is correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Garrett\tNNP\tNN\n",
      "got\tVBD\tVBD\n",
      "his\tPRP$\tPRP$\n",
      "Ph.D.\tNN\tNN\n",
      "from\tIN\tIN\n",
      "the\tDT\tDT\n",
      "Univ\tNNP\tNN\n",
      ".\t.\t.\n",
      "of\tIN\tIN\n",
      "Alberta\tNNP\tNN\n",
      "in\tIN\tIN\n",
      "07/2017\tCD\tCD\n",
      ".\t.\t.\n",
      "Dr.\tNNP\tNNP\n",
      "Nicolai\tNNP\tNN\n",
      "'s\tPOS\tPOS\n",
      "thesis\tNN\tNNS\n",
      "was\tVBD\tVBD\n",
      "too\tRB\tRB\n",
      "long\tRB\tJJ\n",
      ",\t,\t,\n",
      "as\tIN\tIN\n",
      "they\tPRP\tPRP\n",
      "tend\tVBP\tVBP\n",
      "to\tTO\tTO\n",
      "be\tVB\tVB\n",
      ".\t.\t.\n",
      "He\tPRP\tPRP\n",
      "went\tVBD\tVBD\n",
      "to\tTO\tTO\n",
      "the\tDT\tDT\n",
      "U.S.A.\tNNP\tNNP\n",
      "...\t:\t:\n",
      "but\tCC\tCC\n",
      "later\tRB\tJJ\n",
      "came\tVBD\tVBD\n",
      "back\tRB\tRB\n",
      "to\tTO\tTO\n",
      "Canada\tNNP\tNNP\n",
      ".\t.\t.\n"
     ]
    }
   ],
   "source": [
    "for sent in sents:\n",
    "    tokens = word_tokenize(sent)\n",
    "    NLTK_pos = pos_tag(tokens)\n",
    "    our_tags = uni_tagger.tag(tokens)\n",
    "    for i in range(len(tokens)):\n",
    "        print(\"\\t\".join((NLTK_pos[i][0], NLTK_pos[i][1], our_tags[i][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## All-in-one preprocessing with SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "NLTK isn't the only option for preprocessing in English with Python. Another popular choice is [SpaCy](https://spacy.io), which will do everything you might need in one line of code. You'll need to install the package and its models. It's fast and lightweight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!{sys.executable} -m install spacy\n",
    "#!{sys.executable} -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"Hi there. How are you?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hi there. How are you?"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can access what you need in the resulting document object. The sentences are in sents, each sentence is a list of tokens, and each token has a lemmas_, pos_ (the universal POS tag), and tag_ attribute (the treebank pos tag)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there.\n",
      "token: Hi\n",
      "lemma: hi\n",
      "Univeral POS: INTJ\n",
      "PTB POS: UH\n",
      "-----\n",
      "token: there\n",
      "lemma: there\n",
      "Univeral POS: ADV\n",
      "PTB POS: RB\n",
      "-----\n",
      "token: .\n",
      "lemma: .\n",
      "Univeral POS: PUNCT\n",
      "PTB POS: .\n",
      "-----\n",
      "How are you?\n",
      "token: How\n",
      "lemma: how\n",
      "Univeral POS: SCONJ\n",
      "PTB POS: WRB\n",
      "-----\n",
      "token: are\n",
      "lemma: be\n",
      "Univeral POS: AUX\n",
      "PTB POS: VBP\n",
      "-----\n",
      "token: you\n",
      "lemma: you\n",
      "Univeral POS: PRON\n",
      "PTB POS: PRP\n",
      "-----\n",
      "token: ?\n",
      "lemma: ?\n",
      "Univeral POS: PUNCT\n",
      "PTB POS: .\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "    for token in sent:\n",
    "        print(\"token:\", token)\n",
    "        print(\"lemma:\", token.lemma_)\n",
    "        print(\"Univeral POS:\", token.pos_)  \n",
    "        print(\"PTB POS:\", token.tag_)\n",
    "        print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's look at the results for SpaCy and NLTK for preprocessing the sentence and find some difference in the results of their preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "SPACY:  [('Garrett', 'NNP'), ('got', 'VBD'), ('his', 'PRP$'), ('Ph.D.', 'NN'), ('from', 'IN'), ('the', 'DT'), ('Univ', 'NNP'), ('.', 'NNP'), ('of', 'IN'), ('Alberta', 'NNP'), ('in', 'IN'), ('07/2017', 'PRP'), ('.', '.')]\n",
      "NLTK:  [('Garrett', 'NNP'), ('got', 'VBD'), ('his', 'PRP$'), ('Ph.D.', 'NN'), ('from', 'IN'), ('the', 'DT'), ('Univ', 'NNP'), ('.', '.')]\n",
      "SPACY:  [('Dr.', 'NNP'), ('Nicolai', 'NNP'), (\"'s\", 'POS'), ('thesis', 'NN'), ('was', 'VBD'), ('too', 'RB'), ('long', 'JJ'), (',', ','), ('as', 'IN'), ('they', 'PRP'), ('tend', 'VBP'), ('to', 'TO'), ('be', 'VB'), ('.', '.')]\n",
      "NLTK:  [('of', 'IN'), ('Alberta', 'NNP'), ('in', 'IN'), ('07/2017', 'CD'), ('.', '.')]\n",
      "SPACY:  [('He', 'PRP'), ('went', 'VBD'), ('to', 'IN'), ('the', 'DT'), ('U.S.A.', 'NN'), ('...', 'NFP'), ('but', 'CC'), ('later', 'RB'), ('came', 'VBD'), ('back', 'RB'), ('to', 'IN'), ('Canada', 'NNP'), ('.', '.')]\n",
      "NLTK:  [('Dr.', 'NNP'), ('Nicolai', 'NNP'), (\"'s\", 'POS'), ('thesis', 'NN'), ('was', 'VBD'), ('too', 'RB'), ('long', 'RB'), (',', ','), ('as', 'IN'), ('they', 'PRP'), ('tend', 'VBP'), ('to', 'TO'), ('be', 'VB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "spacy_sents = []\n",
    "nltk_sents = []\n",
    "\n",
    "en_doc = nlp(en_text)\n",
    "for sent in en_doc.sents:\n",
    "    spacy_sents.append([(str(token), token.tag_) for token in sent])\n",
    "for sent in sent_tokenize(en_text):\n",
    "    nltk_sents.append(pos_tag(word_tokenize(sent)))\n",
    "\n",
    "print(len(spacy_sents))\n",
    "print(len(nltk_sents))\n",
    "for i in range(len(spacy_sents)):\n",
    "    print(\"SPACY: \", spacy_sents[i])\n",
    "    print(\"NLTK: \", nltk_sents[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "SpaCy also has [multilingual support](https://spacy.io/usage/models#languages). Let's take a quick look at French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considérant que la méconnaissance et le mépris des droits de l'homme ont conduit à des actes de barbarie qui révoltent la conscience de l'humanité et que l'avènement d'un monde où les êtres humains seront libres de parler et de croire, libérés de la terreur et de la misère, a été proclamé comme la plus haute aspiration de l'homme\n",
      "token: Considérant\n",
      "lemma: considérer\n",
      "Universal POS: VERB\n",
      "Language-specific POS: VERB\n",
      "------\n",
      "token: que\n",
      "lemma: que\n",
      "Universal POS: SCONJ\n",
      "Language-specific POS: SCONJ\n",
      "------\n",
      "token: la\n",
      "lemma: le\n",
      "Universal POS: DET\n",
      "Language-specific POS: DET\n",
      "------\n",
      "token: méconnaissance\n",
      "lemma: méconnaissance\n",
      "Universal POS: NOUN\n",
      "Language-specific POS: NOUN\n",
      "------\n",
      "token: et\n",
      "lemma: et\n",
      "Universal POS: CCONJ\n",
      "Language-specific POS: CCONJ\n",
      "------\n",
      "token: le\n",
      "lemma: le\n",
      "Universal POS: DET\n",
      "Language-specific POS: DET\n",
      "------\n",
      "token: mépris\n",
      "lemma: mépris\n",
      "Universal POS: NOUN\n",
      "Language-specific POS: NOUN\n",
      "------\n",
      "token: des\n",
      "lemma: de\n",
      "Universal POS: ADP\n",
      "Language-specific POS: ADP\n",
      "------\n",
      "token: droits\n",
      "lemma: droit\n",
      "Universal POS: NOUN\n",
      "Language-specific POS: NOUN\n",
      "------\n",
      "token: de\n",
      "lemma: de\n",
      "Universal POS: ADP\n",
      "Language-specific POS: ADP\n",
      "------\n",
      "token: l'\n",
      "lemma: le\n",
      "Universal POS: DET\n",
      "Language-specific POS: DET\n",
      "------\n",
      "token: homme\n",
      "lemma: homme\n",
      "Universal POS: NOUN\n",
      "Language-specific POS: NOUN\n",
      "------\n",
      "token: ont\n",
      "lemma: avoir\n",
      "Universal POS: AUX\n",
      "Language-specific POS: AUX\n",
      "------\n",
      "token: conduit\n",
      "lemma: conduire\n",
      "Universal POS: VERB\n",
      "Language-specific POS: VERB\n",
      "------\n",
      "token: à\n",
      "lemma: à\n",
      "Universal POS: ADP\n",
      "Language-specific POS: ADP\n",
      "------\n",
      "token: des\n",
      "lemma: un\n",
      "Universal POS: DET\n",
      "Language-specific POS: DET\n",
      "------\n",
      "token: actes\n",
      "lemma: acte\n",
      "Universal POS: NOUN\n",
      "Language-specific POS: NOUN\n",
      "------\n",
      "token: de\n",
      "lemma: de\n",
      "Universal POS: ADP\n",
      "Language-specific POS: ADP\n",
      "------\n",
      "token: barbarie\n",
      "lemma: barbarie\n",
      "Universal POS: NOUN\n",
      "Language-specific POS: NOUN\n",
      "------\n",
      "token: qui\n",
      "lemma: qui\n",
      "Universal POS: PRON\n",
      "Language-specific POS: PRON\n",
      "------\n",
      "token: révoltent\n",
      "lemma: révolter\n",
      "Universal POS: VERB\n",
      "Language-specific POS: VERB\n",
      "------\n",
      "token: la\n",
      "lemma: le\n",
      "Universal POS: DET\n",
      "Language-specific POS: DET\n",
      "------\n",
      "token: conscience\n",
      "lemma: conscience\n",
      "Universal POS: NOUN\n",
      "Language-specific POS: NOUN\n",
      "------\n",
      "token: de\n",
      "lemma: de\n",
      "Universal POS: ADP\n",
      "Language-specific POS: ADP\n",
      "------\n",
      "token: l'\n",
      "lemma: le\n",
      "Universal POS: DET\n",
      "Language-specific POS: DET\n",
      "------\n",
      "token: humanité\n",
      "lemma: humanité\n",
      "Universal POS: NOUN\n",
      "Language-specific POS: NOUN\n",
      "------\n",
      "token: et\n",
      "lemma: et\n",
      "Universal POS: CCONJ\n",
      "Language-specific POS: CCONJ\n",
      "------\n",
      "token: que\n",
      "lemma: que\n",
      "Universal POS: SCONJ\n",
      "Language-specific POS: SCONJ\n",
      "------\n",
      "token: l'\n",
      "lemma: le\n",
      "Universal POS: DET\n",
      "Language-specific POS: DET\n",
      "------\n",
      "token: avènement\n",
      "lemma: avènement\n",
      "Universal POS: NOUN\n",
      "Language-specific POS: NOUN\n",
      "------\n",
      "token: d'\n",
      "lemma: de\n",
      "Universal POS: ADP\n",
      "Language-specific POS: ADP\n",
      "------\n",
      "token: un\n",
      "lemma: un\n",
      "Universal POS: DET\n",
      "Language-specific POS: DET\n",
      "------\n",
      "token: monde\n",
      "lemma: monde\n",
      "Universal POS: NOUN\n",
      "Language-specific POS: NOUN\n",
      "------\n",
      "token: où\n",
      "lemma: où\n",
      "Universal POS: PRON\n",
      "Language-specific POS: PRON\n",
      "------\n",
      "token: les\n",
      "lemma: le\n",
      "Universal POS: DET\n",
      "Language-specific POS: DET\n",
      "------\n",
      "token: êtres\n",
      "lemma: être\n",
      "Universal POS: NOUN\n",
      "Language-specific POS: NOUN\n",
      "------\n",
      "token: humains\n",
      "lemma: humain\n",
      "Universal POS: ADJ\n",
      "Language-specific POS: ADJ\n",
      "------\n",
      "token: seront\n",
      "lemma: être\n",
      "Universal POS: AUX\n",
      "Language-specific POS: AUX\n",
      "------\n",
      "token: libres\n",
      "lemma: libre\n",
      "Universal POS: ADJ\n",
      "Language-specific POS: ADJ\n",
      "------\n",
      "token: de\n",
      "lemma: de\n",
      "Universal POS: ADP\n",
      "Language-specific POS: ADP\n",
      "------\n",
      "token: parler\n",
      "lemma: parler\n",
      "Universal POS: VERB\n",
      "Language-specific POS: VERB\n",
      "------\n",
      "token: et\n",
      "lemma: et\n",
      "Universal POS: CCONJ\n",
      "Language-specific POS: CCONJ\n",
      "------\n",
      "token: de\n",
      "lemma: de\n",
      "Universal POS: ADP\n",
      "Language-specific POS: ADP\n",
      "------\n",
      "token: croire\n",
      "lemma: croire\n",
      "Universal POS: VERB\n",
      "Language-specific POS: VERB\n",
      "------\n",
      "token: ,\n",
      "lemma: ,\n",
      "Universal POS: PUNCT\n",
      "Language-specific POS: PUNCT\n",
      "------\n",
      "token: libérés\n",
      "lemma: libérer\n",
      "Universal POS: VERB\n",
      "Language-specific POS: VERB\n",
      "------\n",
      "token: de\n",
      "lemma: de\n",
      "Universal POS: ADP\n",
      "Language-specific POS: ADP\n",
      "------\n",
      "token: la\n",
      "lemma: le\n",
      "Universal POS: DET\n",
      "Language-specific POS: DET\n",
      "------\n",
      "token: terreur\n",
      "lemma: terreur\n",
      "Universal POS: NOUN\n",
      "Language-specific POS: NOUN\n",
      "------\n",
      "token: et\n",
      "lemma: et\n",
      "Universal POS: CCONJ\n",
      "Language-specific POS: CCONJ\n",
      "------\n",
      "token: de\n",
      "lemma: de\n",
      "Universal POS: ADP\n",
      "Language-specific POS: ADP\n",
      "------\n",
      "token: la\n",
      "lemma: le\n",
      "Universal POS: DET\n",
      "Language-specific POS: DET\n",
      "------\n",
      "token: misère\n",
      "lemma: misère\n",
      "Universal POS: NOUN\n",
      "Language-specific POS: NOUN\n",
      "------\n",
      "token: ,\n",
      "lemma: ,\n",
      "Universal POS: PUNCT\n",
      "Language-specific POS: PUNCT\n",
      "------\n",
      "token: a\n",
      "lemma: avoir\n",
      "Universal POS: AUX\n",
      "Language-specific POS: AUX\n",
      "------\n",
      "token: été\n",
      "lemma: être\n",
      "Universal POS: AUX\n",
      "Language-specific POS: AUX\n",
      "------\n",
      "token: proclamé\n",
      "lemma: proclamer\n",
      "Universal POS: VERB\n",
      "Language-specific POS: VERB\n",
      "------\n",
      "token: comme\n",
      "lemma: comme\n",
      "Universal POS: ADP\n",
      "Language-specific POS: ADP\n",
      "------\n",
      "token: la\n",
      "lemma: le\n",
      "Universal POS: DET\n",
      "Language-specific POS: DET\n",
      "------\n",
      "token: plus\n",
      "lemma: plus\n",
      "Universal POS: ADV\n",
      "Language-specific POS: ADV\n",
      "------\n",
      "token: haute\n",
      "lemma: haute\n",
      "Universal POS: DET\n",
      "Language-specific POS: DET\n",
      "------\n",
      "token: aspiration\n",
      "lemma: aspiration\n",
      "Universal POS: NOUN\n",
      "Language-specific POS: NOUN\n",
      "------\n",
      "token: de\n",
      "lemma: de\n",
      "Universal POS: ADP\n",
      "Language-specific POS: ADP\n",
      "------\n",
      "token: l'\n",
      "lemma: le\n",
      "Universal POS: DET\n",
      "Language-specific POS: DET\n",
      "------\n",
      "token: homme\n",
      "lemma: homme\n",
      "Universal POS: NOUN\n",
      "Language-specific POS: NOUN\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "#!{sys.executable} -m spacy download fr_core_news_sm\n",
    "\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "doc = nlp(\"Considérant que la méconnaissance et le mépris des droits de l'homme ont conduit à des actes de barbarie qui révoltent la conscience de l'humanité et que l'avènement d'un monde où les êtres humains seront libres de parler et de croire, libérés de la terreur et de la misère, a été proclamé comme la plus haute aspiration de l'homme\")\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "    for token in sent:\n",
    "        print(\"token:\", token)\n",
    "        print(\"lemma:\", token.lemma_)\n",
    "        print(\"Universal POS:\", token.pos_)  \n",
    "        print(\"Language-specific POS:\", token.tag_)\n",
    "        print(\"------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "SpaCy is solid and stable, but if you want the very best, state-of-the-art NLP tools and are willing to sacrifice speed, you might also try [Flair](https://github.com/zalandoresearch/flair). [Textblob](https://textblob.readthedocs.io/en/dev/) is another popular one-stop option for NLP pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T/F questions\n",
    "\n",
    "1. The only reason English sentence segmentation is challenging is that the period is ambiguous. (F)\n",
    "2. Word tokenization for Chinese is easier than English, because Chinese characters correspond directly to words, whereas English has clitics. (F)\n",
    "3. After you stem a word, what is left is also a word. (F) What about lemmatization? (T)\n",
    "4. The morphology of a word (its affixes) give you a lot of information about what its part of speech is (T)\n",
    "5. If you are doing both lemmatization and POS tagging, you should do POS tagging first. (T)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
