{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: nltk.RegexpParser\n",
    "\n",
    "In this notebook, we will learn how to extract noun phrase chunks using the NLTK `RegexParser` class.\n",
    "\n",
    "`RegexParser` can be used to match phrases like NPs using regular expressions on POS tags so let's start by making sure that we can POS tag sentences.  \n",
    "\n",
    "## 1. A simple regex parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('dog', 'NN'), ('saw', 'VBD'), ('my', 'PRP$'), ('cat', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "tagged_sent = pos_tag(word_tokenize(\"The dog saw my cat\"))\n",
    "print(tagged_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`word_tokenize()` is how we make the string into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'dog', 'saw', 'my', 'cat']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"The dog saw my cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then need to import the `RegexpParser` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import RegexpParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining a very simple chunker which recognizes **NPs consisting of a single determiner or possessive pronoun followed by noun**.\n",
    "\n",
    "The syntax for defining chunks is the following:\n",
    "\n",
    "```\n",
    "phrase_type: {regular expression on POS tags}\n",
    "```\n",
    "\n",
    "In the regular expressions, **you will need to surround POS tags using angle brackets** `<...>`, for example `<NN>`.\n",
    "\n",
    "We can apply our chunker on a POS tagged sentence like `tagged_sent` using the `RegexpParser.parse()` function. It will return an NLTK syntax tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP The/DT dog/NN) saw/VBD (NP my/PRP$ cat/NN))\n"
     ]
    }
   ],
   "source": [
    "simple_chunker = RegexpParser(\"NP: {<DT|PRP\\$><NN>}\")  # PRP$ means possessive pronoun, but we need \\ to escape the $\n",
    "print(simple_chunker.parse(tagged_sent)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our chunked sentence in a variable `chunked_sent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_sent = simple_chunker.parse(tagged_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now **extract chunks from `chunked_sent`**. The aim is to form a list of NP chunks `[\"The dog\", \"my cat\"]`.\n",
    "\n",
    "We can access all the nested phrases in our chunk tree using `Tree.subtrees()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP The/DT dog/NN) saw/VBD (NP my/PRP$ cat/NN))\n",
      "(NP The/DT dog/NN)\n",
      "(NP my/PRP$ cat/NN)\n"
     ]
    }
   ],
   "source": [
    "for phrase in chunked_sent.subtrees():\n",
    "    print(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `subtrees()` function will print all nested trees which includes te sentence itself. We're only interested in the noun phrases which have label `NP`. Additionally, we want to extract the chunks as strings, so we will use the `Tree.leaves()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('dog', 'NN')]\n",
      "[('my', 'PRP$'), ('cat', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "for phrase in chunked_sent.subtrees():\n",
    "    if phrase.label() == \"NP\":\n",
    "        print(phrase.leaves())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks almost correct. The only problem is that the leaves here are pairs `(word, pos_tag)` so let's extract the words and joint those into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dog\n",
      "my cat\n"
     ]
    }
   ],
   "source": [
    "for phrase in chunked_sent.subtrees():\n",
    "    if phrase.label() == \"NP\":\n",
    "        leaves = phrase.leaves()\n",
    "        words = [word for word, tag in phrase]\n",
    "        print(\" \".join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a function which returns the noun phrase chunks in a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The boy', 'a cat']\n"
     ]
    }
   ],
   "source": [
    "def get_chunks(sent, chunker):\n",
    "    # First tokenize and POS tag the sentence and extract NP chunks. \n",
    "    tagged_sent = pos_tag(word_tokenize(sent))\n",
    "    chunked_sent = chunker.parse(tagged_sent)\n",
    "    \n",
    "    chunks = []\n",
    "    for phrase in chunked_sent.subtrees():\n",
    "        if phrase.label() == \"NP\":\n",
    "            tagged_chunk = phrase.leaves()\n",
    "            words = [word for word, tag in tagged_chunk]\n",
    "            chunks.append(\" \".join(words))\n",
    "    return chunks\n",
    "\n",
    "print(get_chunks(\"The boy saw a cat\", simple_chunker))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice\n",
    "\n",
    "Define a chunker for adjective phrases `AdjP` called `adj_chunker`. An adjective phrase should should contain an adjective `JJ`, `JJR` or `JJS` which can be preceded by an optional adverb `RB`. You can indicate optionality using the question mark operator, i.e. `<TAG>?` or use a disjunction `EXPR1|EXPR2`. You can also use parentheses as needed.\n",
    "\n",
    "Test your chunker using the sentence \"The grey cat saw a very large mouse\". Your chunker should find two chunks: \"grey\" and \"very small\". You should tag the sentence using NLTK `pos_tag` and then apply the chunker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  (AdjP grey/JJ)\n",
      "  cat/NN\n",
      "  saw/VBD\n",
      "  a/DT\n",
      "  (AdjP very/RB large/JJ)\n",
      "  mouse/NN)\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "tagged_sent = pos_tag(word_tokenize(\"The grey cat saw a very large mouse\"))\n",
    "simple_chunker = RegexpParser(\"AdjP: {<RB>?<JJ(R|S)?>}\")\n",
    "print(simple_chunker.parse(tagged_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Applying RegexParser to a corpus\n",
    "\n",
    "NLTK offers a **corpus which has gold standard chunks prepared by linguists**. It is the `conll2000` chunking shared task  dataset.\n",
    "\n",
    "Let's start by downloading the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2000 to /Users/lxy/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/conll2000.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('conll2000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `conll2000` corpus **contains verb phrase (VP) and prepositional phrase (PP) chunks in addition to noun phrase chunks**. You can extract all chunks using the command `conll2000.chunked_sents()` but we will only need NP chunks now so we'll use a **`chunk_types=[\"NP\"]` parameter** to indicate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2000\n",
    "\n",
    "gold_chunked_sents = conll2000.chunked_sents(chunk_types=[\"NP\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print **the first chunked sentence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Confidence/NN)\n",
      "  in/IN\n",
      "  (NP the/DT pound/NN)\n",
      "  is/VBZ\n",
      "  widely/RB\n",
      "  expected/VBN\n",
      "  to/TO\n",
      "  take/VB\n",
      "  (NP another/DT sharp/JJ dive/NN)\n",
      "  if/IN\n",
      "  (NP trade/NN figures/NNS)\n",
      "  for/IN\n",
      "  (NP September/NNP)\n",
      "  ,/,\n",
      "  due/JJ\n",
      "  for/IN\n",
      "  (NP release/NN)\n",
      "  (NP tomorrow/NN)\n",
      "  ,/,\n",
      "  fail/VB\n",
      "  to/TO\n",
      "  show/VB\n",
      "  (NP a/DT substantial/JJ improvement/NN)\n",
      "  from/IN\n",
      "  (NP July/NNP and/CC August/NNP)\n",
      "  (NP 's/POS near-record/JJ deficits/NNS)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(gold_chunked_sents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now **apply our `simple_chunker` on the `conll2000` corpus**. Since the `conll2000` corpus contains gold standard POS tags, we can use those when extracting chunks.\n",
    "\n",
    "Let's first get all the **POS tagged sentences from `conll2000`** and save them in a variable `tagged_sents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Confidence', 'NN'), ('in', 'IN'), ('the', 'DT'), ('pound', 'NN'), ('is', 'VBZ'), ('widely', 'RB'), ('expected', 'VBN'), ('to', 'TO'), ('take', 'VB'), ('another', 'DT'), ('sharp', 'JJ'), ('dive', 'NN'), ('if', 'IN'), ('trade', 'NN'), ('figures', 'NNS'), ('for', 'IN'), ('September', 'NNP'), (',', ','), ('due', 'JJ'), ('for', 'IN'), ('release', 'NN'), ('tomorrow', 'NN'), (',', ','), ('fail', 'VB'), ('to', 'TO'), ('show', 'VB'), ('a', 'DT'), ('substantial', 'JJ'), ('improvement', 'NN'), ('from', 'IN'), ('July', 'NNP'), ('and', 'CC'), ('August', 'NNP'), (\"'s\", 'POS'), ('near-record', 'JJ'), ('deficits', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged_sents = conll2000.tagged_sents()\n",
    "print(tagged_sents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's **apply our `simple_chunker` to the first tagged sentence**. \n",
    "\n",
    "The only NP we actually find is `(NP the/DT pound/NN)`. Comparing with the gold standard chunked sentence, we can immediately see two issues:\n",
    "\n",
    "1. The determiner is often optional as in `(NP Confidence/NN)`.\n",
    "1. NPs often contain adjectives like `(NP another/DT sharp/JJ dive/NN)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Confidence/NN\n",
      "  in/IN\n",
      "  the/DT\n",
      "  pound/NN\n",
      "  is/VBZ\n",
      "  widely/RB\n",
      "  expected/VBN\n",
      "  to/TO\n",
      "  take/VB\n",
      "  another/DT\n",
      "  (AdjP sharp/JJ)\n",
      "  dive/NN\n",
      "  if/IN\n",
      "  trade/NN\n",
      "  figures/NNS\n",
      "  for/IN\n",
      "  September/NNP\n",
      "  ,/,\n",
      "  (AdjP due/JJ)\n",
      "  for/IN\n",
      "  release/NN\n",
      "  tomorrow/NN\n",
      "  ,/,\n",
      "  fail/VB\n",
      "  to/TO\n",
      "  show/VB\n",
      "  a/DT\n",
      "  (AdjP substantial/JJ)\n",
      "  improvement/NN\n",
      "  from/IN\n",
      "  July/NNP\n",
      "  and/CC\n",
      "  August/NNP\n",
      "  's/POS\n",
      "  (AdjP near-record/JJ)\n",
      "  deficits/NNS\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(simple_chunker.parse(tagged_sents[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's improve our `simple_chunker` by **making the determiner (`DT`) tag optional and allowing for an optional adjective  (`JJ`) tag** between the determiner and noun. \n",
    "\n",
    "This works substantially better but there are still several cases which are not correctly handled. One is that **our chunker only allows for simple `NN` tags** which means that nouns having `NNS`, `NNP` and `NNPS` tags are missed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Confidence/NN)\n",
      "  in/IN\n",
      "  (NP the/DT pound/NN)\n",
      "  is/VBZ\n",
      "  widely/RB\n",
      "  expected/VBN\n",
      "  to/TO\n",
      "  take/VB\n",
      "  (NP another/DT sharp/JJ dive/NN)\n",
      "  if/IN\n",
      "  (NP trade/NN)\n",
      "  figures/NNS\n",
      "  for/IN\n",
      "  September/NNP\n",
      "  ,/,\n",
      "  due/JJ\n",
      "  for/IN\n",
      "  (NP release/NN)\n",
      "  (NP tomorrow/NN)\n",
      "  ,/,\n",
      "  fail/VB\n",
      "  to/TO\n",
      "  show/VB\n",
      "  (NP a/DT substantial/JJ improvement/NN)\n",
      "  from/IN\n",
      "  July/NNP\n",
      "  and/CC\n",
      "  August/NNP\n",
      "  's/POS\n",
      "  near-record/JJ\n",
      "  deficits/NNS\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "simple_chunker = RegexpParser(\"NP: {<DT>?<JJ>?<NN>}\")\n",
    "\n",
    "print(simple_chunker.parse(tagged_sents[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now further extend our NP chunker by allowing for any noun tag. The easiest way to do this is to notice that noun tags in the Penn Treebank POS tagset are exactly the tags which start with `NN`. We can model noun tags using the regular expression `NN.*` which allows for an arbitrary combination of characters after `NN`. \n",
    "\n",
    "We will also allow several nouns in the NP to account for cases like \"budget deficit\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Confidence/NN)\n",
      "  in/IN\n",
      "  (NP the/DT pound/NN)\n",
      "  is/VBZ\n",
      "  widely/RB\n",
      "  expected/VBN\n",
      "  to/TO\n",
      "  take/VB\n",
      "  (NP another/DT sharp/JJ dive/NN)\n",
      "  if/IN\n",
      "  (NP trade/NN figures/NNS)\n",
      "  for/IN\n",
      "  (NP September/NNP)\n",
      "  ,/,\n",
      "  due/JJ\n",
      "  for/IN\n",
      "  (NP release/NN tomorrow/NN)\n",
      "  ,/,\n",
      "  fail/VB\n",
      "  to/TO\n",
      "  show/VB\n",
      "  (NP a/DT substantial/JJ improvement/NN)\n",
      "  from/IN\n",
      "  (NP July/NNP)\n",
      "  and/CC\n",
      "  (NP August/NNP)\n",
      "  's/POS\n",
      "  (NP near-record/JJ deficits/NNS)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "simple_chunker = RegexpParser(\"NP: {<DT>?<JJ>?<NN.*>+}\")\n",
    "\n",
    "print(simple_chunker.parse(tagged_sents[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'd like to compare the chunks identified by our chunker to the gold standard noun phrase chunks in the `conll2000` corpus. There is a builtin function in `RegexpParser` which computes both IOB tag accuracy and F1-score for us. It takes the gold standard chunked `conll2000` corpus as input.\n",
    "\n",
    "We can see that while we do identify quite a few correct chunks, we could definitely do better given the 64.1% F1-score. You can continue to develop better regex chunkers :)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  79.3%%\n",
      "    Precision:     69.2%%\n",
      "    Recall:        59.6%%\n",
      "    F-Measure:     64.1%%\n"
     ]
    }
   ],
   "source": [
    "print(simple_chunker.evaluate(gold_chunked_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
