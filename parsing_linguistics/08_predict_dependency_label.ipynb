{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8: Predicting dependency labels\n",
    "\n",
    "Today we will assume that we have some mechanism for building dependency graphs, either transition-based or graph-based. We will now **focus on the task of predicting dependency labels** and we'll use a logistic regression model to do this.\n",
    "\n",
    "![unk deps](unk_dep.png)\n",
    "\n",
    "\n",
    "## 1. Labeled dependency treebank\n",
    "\n",
    "Unfortunately, the dependency treebank which is distributed with NLTK does not contain actual dependency labels. You can, however, convert the phrase structure trees in Penn treebank into labeled dependency trees. You will need to install the `PyStanfordDependencies` toolkit first:\n",
    "```\n",
    "pip3 install PyStanfordDependencies\n",
    "```\n",
    "\n",
    "Here's code for convering the Penn Treebank into a **labeled** dependency treebank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Warning: it's going to take a long while to run this\n",
    "import StanfordDependencies\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "sd = StanfordDependencies.get_instance(backend='subprocess')\n",
    "parse_trees = list(treebank.parsed_sents())\n",
    "\n",
    "dep_treebank = []\n",
    "for i,sent in enumerate(parse_trees):\n",
    "    print(i)\n",
    "    dep_treebank.append(sd.convert_tree(str(sent)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, `PyStanfordDependencies` is quite slow so I have preconverted the 1000 first sentences and stored them as a Pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "dep_treebank = pickle.load(open(\"dep_treebank.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by examining the treebank. Each sentence is a list of entries like:\n",
    "```\n",
    "Token(index=1, form='Meanwhile', cpos='RB', pos='RB', head=7, deprel='advmod')\n",
    "```\n",
    "which specify a token in the sentence, its POS, head and the dependency relation between the token and head.\n",
    "\n",
    "Indexing for word tokens starts from `1` and the syntactic head of the entire sentence has `head=0`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "Token(index=1, form='Meanwhile', cpos='RB', pos='RB', head=7, deprel='advmod')\n",
      "Token(index=2, form=',', cpos=',', pos=',', head=7, deprel='punct')\n",
      "Token(index=3, form='business', cpos='NN', pos='NN', head=6, deprel='compound')\n",
      "Token(index=4, form='and', cpos='CC', pos='CC', head=3, deprel='cc')\n",
      "Token(index=5, form='government', cpos='NN', pos='NN', head=3, deprel='conj')\n",
      "Token(index=6, form='leaders', cpos='NNS', pos='NNS', head=7, deprel='nsubj')\n",
      "Token(index=7, form='rebuked', cpos='VBD', pos='VBD', head=0, deprel='root')\n",
      "Token(index=8, form='the', cpos='DT', pos='DT', head=10, deprel='det')\n",
      "Token(index=9, form='computer', cpos='NN', pos='NN', head=10, deprel='compound')\n",
      "Token(index=10, form='makers', cpos='NNS', pos='NNS', head=7, deprel='dobj')\n",
      "Token(index=11, form=',', cpos=',', pos=',', head=7, deprel='punct')\n",
      "Token(index=12, form='and', cpos='CC', pos='CC', head=7, deprel='cc')\n",
      "Token(index=13, form='fretted', cpos='VBD', pos='VBD', head=7, deprel='conj')\n",
      "Token(index=14, form='about', cpos='IN', pos='IN', head=17, deprel='case')\n",
      "Token(index=15, form='the', cpos='DT', pos='DT', head=17, deprel='det')\n",
      "Token(index=16, form='broader', cpos='JJR', pos='JJR', head=17, deprel='amod')\n",
      "Token(index=17, form='statement', cpos='NN', pos='NN', head=13, deprel='nmod')\n",
      "Token(index=18, form='the', cpos='DT', pos='DT', head=19, deprel='det')\n",
      "Token(index=19, form='companies', cpos='NNS', pos='NNS', head=21, deprel='nmod:poss')\n",
      "Token(index=20, form=\"'\", cpos='POS', pos='POS', head=19, deprel='case')\n",
      "Token(index=21, form='actions', cpos='NNS', pos='NNS', head=22, deprel='nsubj')\n",
      "Token(index=22, form='make', cpos='VBP', pos='VBP', head=17, deprel='acl:relcl')\n",
      "Token(index=23, form='about', cpos='IN', pos='IN', head=26, deprel='case')\n",
      "Token(index=24, form='Japanese', cpos='JJ', pos='JJ', head=26, deprel='amod')\n",
      "Token(index=25, form='cutthroat', cpos='JJ', pos='JJ', head=26, deprel='amod')\n",
      "Token(index=26, form='pricing', cpos='NN', pos='NN', head=17, deprel='nmod')\n",
      "Token(index=27, form='.', cpos='.', pos='.', head=7, deprel='punct')\n"
     ]
    }
   ],
   "source": [
    "print(len(dep_treebank))\n",
    "\n",
    "for token in dep_treebank[0]:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the features of individual tokens in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meanwhile RB\n"
     ]
    }
   ],
   "source": [
    "print(dep_treebank[0][0].form,dep_treebank[0][0].pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now create a function which splits our dataset into a training, development and test set. We'll use 10% of the data respectively for testing and development. The rest is used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    train_set = []\n",
    "    dev_set = []\n",
    "    test_set = []\n",
    "    \n",
    "    for i, sent in enumerate(data):\n",
    "        if i % 10 == 0:\n",
    "            test_set.append(sent)\n",
    "        elif i % 10 == 1:\n",
    "            dev_set.append(sent)\n",
    "        else:\n",
    "            train_set.append(sent)\n",
    "            \n",
    "    return train_set, dev_set, test_set\n",
    "\n",
    "train_set, dev_set, test_set = split_data(dep_treebank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that we get the correct lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 100 100\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set), len(dev_set), len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature extraction\n",
    "\n",
    "Next we'll build a feature extraction function which uses the following features to predict the label of the dependency arc between a dependent and its head:\n",
    "\n",
    "* the word form of the depedent\n",
    "* the POS tag of the dependent\n",
    "* the word form of the head\n",
    "* the POS tag of the head\n",
    "* a dummy feature which is always active\n",
    "\n",
    "We will also return the gold standard labels for each dependency relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_features(data):\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for sent in data:\n",
    "        for example in sent:\n",
    "            dep_wf = example.form\n",
    "            dep_pos = example.pos\n",
    "            \n",
    "            # default set\n",
    "            head_wf = \"ROOT\"\n",
    "            head_pos = \"TOP\"\n",
    "            if example.head > 0:   # if the word is not the head of the sentence\n",
    "                head_wf = sent[example.head-1].form    # the index starts with 1, so we have to minus 1 here\n",
    "                head_pos = sent[example.head-1].pos\n",
    "        \n",
    "            features.append({\"DEP_WF\":dep_wf,\n",
    "                             \"DEP_POS\":dep_pos,\n",
    "                             \"HEAD_WF\": head_wf,\n",
    "                             \"HEAD_POS\":head_pos})\n",
    "            deprel = example.deprel\n",
    "            labels.append(deprel)\n",
    "    return features, labels\n",
    "        \n",
    "train_features, train_labels = extract_features(train_set)\n",
    "dev_features, dev_labels = extract_features(dev_set)\n",
    "test_features, test_labels = extract_features(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the sizes of the datasets to check that everything looks okay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19721 2591 2635\n"
     ]
    }
   ],
   "source": [
    "print(len(train_features),len(dev_features),len(test_features))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the features for the first training example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DEP_WF': 'The', 'DEP_POS': 'DT', 'HEAD_WF': 'company', 'HEAD_POS': 'NN'}\n"
     ]
    }
   ],
   "source": [
    "print(train_features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will need to convert our feature dictionaries into numerical feature vectors (of zeroes and ones). We will do this using the `sklearn` [DictVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "vectorizer.fit(train_features)\n",
    "\n",
    "# transform everything using the same vectorizer trained from the `train_features`\n",
    "train_X = vectorizer.transform(train_features)\n",
    "dev_X = vectorizer.transform(dev_features)\n",
    "test_X = vectorizer.transform(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the shapes of our output matrices to check that everything looks okay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19721, 7477) (2591, 7477) (2635, 7477)\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape,dev_X.shape,test_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to numericalize our label vectors. We can do this using the `sklearn` [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_labels+dev_labels+test_labels)\n",
    "\n",
    "train_y = label_encoder.transform(train_labels)\n",
    "dev_y = label_encoder.transform(dev_labels)\n",
    "test_y = label_encoder.transform(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We turn every vector into some lables (integers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18 31  3 ... 18 27 35]\n",
      "19721\n"
     ]
    }
   ],
   "source": [
    "print(train_y)\n",
    "print(len(train_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training a classifier\n",
    "\n",
    "We are then ready to train a logistic regression model. We will use the `sklearn` [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(random_state=0,max_iter=1000).fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then apply the model to our development set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sys_dev_y = lr.predict(dev_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31, 36, 31, ..., 12, 12, 35])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_dev_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the accuracy of the dependency label classifier (it should be around 81%):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8162871478193747"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_acc(sys_y, gold_y):\n",
    "    return (sys_y == gold_y).sum()/len(sys_y)\n",
    "\n",
    "get_acc(sys_dev_y, dev_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Refining the features\n",
    "\n",
    "The features we used were quite simple. A nice boost in accruacy can be achieved by adding the POS tags of the word forms surrounding the dependent because these can often offer a clue about the role that the dependent plays in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_features(data):\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for sent in data:\n",
    "        for example in sent:\n",
    "            dep_wf = example.form\n",
    "            dep_pos = example.pos\n",
    "            \n",
    "            head_wf = \"ROOT\"\n",
    "            head_pos = \"TOP\"\n",
    "            if example.head > 0:\n",
    "                head_wf = sent[example.head-1].form\n",
    "                head_pos = sent[example.head-1].pos\n",
    "        \n",
    "            dep_pos_minus_1 = \"NONE\"\n",
    "            if example.index > 1:  # if it's not the first word\n",
    "                dep_pos_minus_1 = sent[example.index - 2].pos\n",
    "            \n",
    "            dep_pos_plus_1 = \"NONE\"\n",
    "            if example.index != len(sent):  # if it's not the last word\n",
    "                dep_pos_plus_1 = sent[example.index].pos\n",
    "    \n",
    "            features.append({\"DEP_WF\":dep_wf, \n",
    "                             \"DEP_POS\":dep_pos, \n",
    "                             \"HEAD_WF\":head_wf, \n",
    "                             \"HEAD_POS\":head_pos, \n",
    "                             \"DEP_POS_MINUS_1\":dep_pos_minus_1,\n",
    "                             \"DEP_POS_PLUS_1\":dep_pos_plus_1,\n",
    "                             \"DUMMY\":\"DUMMY\"})\n",
    "            deprel = example.deprel\n",
    "            labels.append(deprel)\n",
    "    return features, labels\n",
    "        \n",
    "train_features, train_labels = extract_features(train_set)\n",
    "dev_features, dev_labels = extract_features(dev_set)\n",
    "test_features, test_labels = extract_features(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to re-vectorize our features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer = DictVectorizer()\n",
    "vectorizer.fit(train_features)\n",
    "\n",
    "train_X = vectorizer.transform(train_features)\n",
    "dev_X = vectorizer.transform(dev_features)\n",
    "test_X = vectorizer.transform(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And retrain the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=0,max_iter=1000).fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that accuracy improves substantially (it should be around 89%):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8892319567734466"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_dev_y = lr.predict(dev_X)\n",
    "get_acc(sys_dev_y, dev_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical work\n",
    "\n",
    "A lot still remains to be done with regard to feature extraction. Other features you might want to try are:\n",
    "\n",
    "* word forms and POS tags around the head word\n",
    "* Other dependent word forms and their POS tags for both the head and dependent\n",
    "* The head word for the head of this relation (and its POS tag)\n",
    "\n",
    "It's important that you leverage the graph structure when designing your features.\n",
    "\n",
    "Of course you should also tune the hyperparameters of the `LogisticRegression` classifier on the development set and run a final test on the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
